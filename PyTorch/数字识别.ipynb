{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0d76f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca89ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "learning_rate = 0.01\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc156a54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\torchvision\\datasets\\mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76dc26a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data', train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b7ad6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1 = torch.randn(200, 784, requires_grad=True), \\\n",
    "         torch.zeros(200, requires_grad=True)\n",
    "w2, b2 = torch.randn(200, 200, requires_grad=True), \\\n",
    "         torch.zeros(200, requires_grad=True)\n",
    "w3, b3 = torch.randn(10, 200, requires_grad=True), \\\n",
    "         torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df91066d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0178, -0.0326,  0.1333,  ..., -0.0987,  0.0617,  0.0852],\n",
       "        [ 0.0287,  0.0219,  0.1041,  ...,  0.1376,  0.0067,  0.0987],\n",
       "        [ 0.0376, -0.0200,  0.0045,  ...,  0.0291,  0.0581,  0.1690],\n",
       "        ...,\n",
       "        [-0.0427,  0.0722,  0.0509,  ..., -0.0538,  0.1099,  0.0406],\n",
       "        [ 0.0097,  0.0557,  0.0663,  ...,  0.0235,  0.0015, -0.1474],\n",
       "        [ 0.0170, -0.1264, -0.0780,  ...,  0.0772, -0.2862, -0.1214]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.kaiming_normal_(w1)\n",
    "torch.nn.init.kaiming_normal_(w2)\n",
    "torch.nn.init.kaiming_normal_(w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00c01ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    x = x @ w1.t() + b1\n",
    "    x = F.relu(x)\n",
    "    x = x @ w2.t() + b2\n",
    "    x = F.relu(x)\n",
    "    x = x @ w3.t() + b3\n",
    "    x = F.relu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1adcba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([w1, b1, w2, b2, w3, b3], lr=learning_rate)\n",
    "criteon = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0db9efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.602112\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 1.171249\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.592357\n",
      "\n",
      "Test set: Average loss: 0.0030, Accuracy: 8130/10000 (81%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.744625\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.772680\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.512238\n",
      "\n",
      "Test set: Average loss: 0.0025, Accuracy: 8346/10000 (83%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.416788\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.514242\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.571888\n",
      "\n",
      "Test set: Average loss: 0.0023, Accuracy: 8424/10000 (84%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.436522\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.500814\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.496050\n",
      "\n",
      "Test set: Average loss: 0.0022, Accuracy: 8463/10000 (85%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.402420\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.482642\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.300312\n",
      "\n",
      "Test set: Average loss: 0.0021, Accuracy: 8512/10000 (85%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.437006\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.436026\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.339921\n",
      "\n",
      "Test set: Average loss: 0.0021, Accuracy: 8551/10000 (86%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.399523\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.312888\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.445424\n",
      "\n",
      "Test set: Average loss: 0.0020, Accuracy: 8563/10000 (86%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.378816\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.389168\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.343188\n",
      "\n",
      "Test set: Average loss: 0.0020, Accuracy: 8592/10000 (86%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.354660\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.333931\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.405090\n",
      "\n",
      "Test set: Average loss: 0.0019, Accuracy: 8602/10000 (86%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.356630\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.420714\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.357557\n",
      "\n",
      "Test set: Average loss: 0.0019, Accuracy: 8621/10000 (86%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(-1, 28 * 28)\n",
    "\n",
    "        logits = forward(data)\n",
    "        loss = criteon(logits, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()\n",
    "            ))\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        logits = forward(data)\n",
    "        test_loss += criteon(logits, target).item()\n",
    "\n",
    "        pred = logits.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88344c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc443219",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "learning_rate = 0.01\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f615bf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b16993ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data', train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "105b7ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1, b1 = torch.randn(200, 784, requires_grad=True), torch.zeros(200, requires_grad=True)\n",
    "w2, b2 = torch.randn(200, 200, requires_grad=True), torch.zeros(200, requires_grad=True)\n",
    "w3, b3 = torch.randn(10, 200, requires_grad=True), torch.zeros(10, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0ae2236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1278,  0.0964, -0.1607,  ...,  0.0830,  0.0320, -0.0735],\n",
       "        [-0.0958,  0.1588, -0.1340,  ..., -0.0567,  0.0096, -0.0251],\n",
       "        [ 0.0106,  0.0056,  0.0127,  ...,  0.0120, -0.0457,  0.0119],\n",
       "        ...,\n",
       "        [ 0.1353,  0.0228, -0.0303,  ...,  0.1272, -0.0718, -0.0387],\n",
       "        [ 0.0779,  0.0919, -0.0928,  ..., -0.0181, -0.0562, -0.0630],\n",
       "        [ 0.1140,  0.1295, -0.0261,  ...,  0.0234, -0.0659, -0.0154]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.init.kaiming_normal_(w1)\n",
    "torch.nn.init.kaiming_normal_(w2)\n",
    "torch.nn.init.kaiming_normal_(w3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46d00119",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x):\n",
    "    x = x @ w1.t() + b1\n",
    "    x = F.relu(x)\n",
    "    x = x @ w2.t() + b2\n",
    "    x = F.relu(x)\n",
    "    x = x @ w3.t() + b3\n",
    "    x = F.relu(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "797b1a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD([w1, b1, w2, b2, w3, b3], lr=learning_rate)\n",
    "criteon = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1ef6097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.795672\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.648840\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.445855\n",
      "\n",
      "Test set: Average loss: 0.0018, Accuracy: 8988/10000 (90%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.301157\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.351397\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.373636\n",
      "\n",
      "Test set: Average loss: 0.0014, Accuracy: 9178/10000 (92%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.224690\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.187569\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.179643\n",
      "\n",
      "Test set: Average loss: 0.0012, Accuracy: 9284/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.326483\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.287169\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.200313\n",
      "\n",
      "Test set: Average loss: 0.0011, Accuracy: 9344/10000 (93%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.227843\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.134467\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.170584\n",
      "\n",
      "Test set: Average loss: 0.0010, Accuracy: 9389/10000 (94%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.132782\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.189013\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.223220\n",
      "\n",
      "Test set: Average loss: 0.0009, Accuracy: 9429/10000 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.186160\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.242385\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.132748\n",
      "\n",
      "Test set: Average loss: 0.0009, Accuracy: 9472/10000 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.148862\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.183563\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.101346\n",
      "\n",
      "Test set: Average loss: 0.0008, Accuracy: 9504/10000 (95%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.089189\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.128635\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.170360\n",
      "\n",
      "Test set: Average loss: 0.0008, Accuracy: 9517/10000 (95%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.103667\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.144558\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.159675\n",
      "\n",
      "Test set: Average loss: 0.0008, Accuracy: 9553/10000 (96%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(-1, 28 * 28)\n",
    "\n",
    "        logits = forward(data)\n",
    "        loss = criteon(logits, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()\n",
    "            ))\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        logits = forward(data)\n",
    "        test_loss += criteon(logits, target).item()\n",
    "\n",
    "        pred = logits.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9197c27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f83ef14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "learning_rate = 0.01\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a57d12ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 200),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(200, 10),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d918b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MLP()\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "criteon = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b2fe807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.306177\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 2.095823\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 1.733240\n",
      "\n",
      "Test set: Average loss: 0.0074, Accuracy: 5379/10000 (54%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 1.555467\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 1.400395\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 1.257120\n",
      "\n",
      "Test set: Average loss: 0.0062, Accuracy: 5559/10000 (56%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.195915\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 1.036250\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 1.280865\n",
      "\n",
      "Test set: Average loss: 0.0059, Accuracy: 5589/10000 (56%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 1.224541\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.122374\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.208355\n",
      "\n",
      "Test set: Average loss: 0.0054, Accuracy: 6251/10000 (63%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.142502\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.980136\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 1.218428\n",
      "\n",
      "Test set: Average loss: 0.0050, Accuracy: 6324/10000 (63%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.026705\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.949556\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 1.161186\n",
      "\n",
      "Test set: Average loss: 0.0048, Accuracy: 6378/10000 (64%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.784183\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 1.077450\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.960380\n",
      "\n",
      "Test set: Average loss: 0.0047, Accuracy: 6406/10000 (64%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.920040\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 1.096686\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.986749\n",
      "\n",
      "Test set: Average loss: 0.0046, Accuracy: 6437/10000 (64%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.960054\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.867765\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.946388\n",
      "\n",
      "Test set: Average loss: 0.0046, Accuracy: 6477/10000 (65%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.885439\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.838613\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.834679\n",
      "\n",
      "Test set: Average loss: 0.0045, Accuracy: 6489/10000 (65%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(-1, 28 * 28)\n",
    "\n",
    "        logits = net(data)\n",
    "        loss = criteon(logits, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()\n",
    "            ))\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        logits = net(data)\n",
    "        test_loss += criteon(logits, target).item()\n",
    "\n",
    "        pred = logits.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e2d7c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a4773a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "learning_rate = 0.01\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8d9c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d737d017",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "21dc696f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(784, 200),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(200, 200),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "            nn.Linear(200, 10),\n",
    "            nn.LeakyReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "578e81b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')\n",
    "net = MLP().to(device)\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate)\n",
    "criteon = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ff040a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.299660\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 2.071889\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 1.383904\n",
      "\n",
      "Test set: Average loss: 0.0038, Accuracy: 8415/10000 (84%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.790846\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.569397\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.653322\n",
      "\n",
      "Test set: Average loss: 0.0020, Accuracy: 8900/10000 (89%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.393598\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.365815\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.359918\n",
      "\n",
      "Test set: Average loss: 0.0017, Accuracy: 9038/10000 (90%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.370012\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.338194\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.286320\n",
      "\n",
      "Test set: Average loss: 0.0015, Accuracy: 9115/10000 (91%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.255046\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.273912\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.319165\n",
      "\n",
      "Test set: Average loss: 0.0014, Accuracy: 9188/10000 (92%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.380370\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.303537\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.169834\n",
      "\n",
      "Test set: Average loss: 0.0013, Accuracy: 9243/10000 (92%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.202995\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.209689\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.249326\n",
      "\n",
      "Test set: Average loss: 0.0012, Accuracy: 9286/10000 (93%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.213367\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 0.214813\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.242899\n",
      "\n",
      "Test set: Average loss: 0.0012, Accuracy: 9317/10000 (93%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.245582\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.160554\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 0.237978\n",
      "\n",
      "Test set: Average loss: 0.0011, Accuracy: 9345/10000 (93%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.153508\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 0.244536\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.242309\n",
      "\n",
      "Test set: Average loss: 0.0011, Accuracy: 9379/10000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        data, target = data.to(device), target.cuda()\n",
    "\n",
    "        logits = net(data)\n",
    "        loss = criteon(logits, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()\n",
    "            ))\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data = data.view(-1, 28 * 28)\n",
    "        data, target = data.to(device), target.cuda()\n",
    "        logits = net(data)\n",
    "        test_loss += criteon(logits, target).item()\n",
    "\n",
    "        pred = logits.data.max(1)[1]\n",
    "        correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)\n",
    "    ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
